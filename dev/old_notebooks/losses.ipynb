{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Title\n",
    "- When you feel you need to use function in general, then there is a natural barier here and you should write it into script and import it\n",
    "\n",
    "<!--- ![alt text](/home/patrik/pcflow/drawio/images/code.drawio.png) -->\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from vis.deprecated_vis import *\n",
    "import glob\n",
    "from data.PATHS import DATA_PATH, TMP_VIS_PATH\n",
    "from loss.flow import *\n",
    "\n",
    "import argparse"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T15:24:44.676822457Z",
     "start_time": "2023-06-12T15:24:40.091697932Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Structure Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Case for NN Flow\n",
    "Argoverse1 NSF - frame 2\n",
    "<img src=\"/home/patrik/pcflow/use_cases/Smooth_NN_from_flow.png\" width=600>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "N1= 50\n",
    "N2 = 55\n",
    "K = 3\n",
    "# na BS = 1\n",
    "pc1 = torch.rand(1, N1, 3)\n",
    "pc2 = torch.rand(1, N2, 3)\n",
    "est_flow = torch.rand(1, N1, 3, requires_grad=True)\n",
    "\n",
    "VOF, HOF = 100, 100 # from dataset\n",
    "_, nn_ind, _ = knn_points(pc1, pc1, lengths1=None, lengths2=None, K=K, norm=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:45.400942448Z",
     "start_time": "2023-06-12T13:58:45.397450399Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8669, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def chamfer_distance_loss(x, y, x_lengths=None, y_lengths=None, both_ways=False, use_normals=False, normals_K=5, loss_norm=1):\n",
    "    '''\n",
    "    Unique Nearest Neightboors?\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :param x_lengths:\n",
    "    :param y_lengths:\n",
    "    :param reduction:\n",
    "    :return:\n",
    "    '''\n",
    "    if use_normals:\n",
    "        normals1 = estimate_pointcloud_normals(x, neighborhood_size=normals_K)\n",
    "        normals2 = estimate_pointcloud_normals(y, neighborhood_size=normals_K)\n",
    "\n",
    "        x = torch.cat([x, normals1], dim=-1)\n",
    "        y = torch.cat([y, normals2], dim=-1)\n",
    "\n",
    "\n",
    "    x_nn = knn_points(x, y, lengths1=x_lengths, lengths2=y_lengths, K=1, norm=loss_norm)\n",
    "    cham_x = x_nn.dists[..., 0]  # (N, P1)\n",
    "    x_nearest_to_y = x_nn[1]\n",
    "\n",
    "    if both_ways:\n",
    "        y_nn = knn_points(y, x, lengths1=y_lengths, lengths2=x_lengths, K=1, norm=loss_norm)\n",
    "        cham_y = y_nn.dists[..., 0]  # (N, P2)\n",
    "        y_nearest_to_x = y_nn[1]\n",
    "\n",
    "        nn_loss = (cham_x.mean() + cham_y.mean() ) / 2 # different shapes\n",
    "\n",
    "    else:\n",
    "        nn_loss = cham_x.mean()\n",
    "\n",
    "    return nn_loss, cham_x, x_nearest_to_y\n",
    "\n",
    "chamf_loss, per_point, x_to_y = chamfer_distance_loss(pc1 + est_flow, pc2, both_ways=False, use_normals=True, normals_K=K, loss_norm=1)\n",
    "\n",
    "print(chamf_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:45.449708904Z",
     "start_time": "2023-06-12T13:58:45.397804952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0814)\n"
     ]
    }
   ],
   "source": [
    "def smoothness_loss(est_flow, NN_idx, loss_norm=1, mask=None):\n",
    "\n",
    "    bs, n, c = est_flow.shape\n",
    "\n",
    "    if bs > 1:\n",
    "        print(\"Smoothness Maybe not working, needs testing!\")\n",
    "    K = NN_idx.shape[2]\n",
    "\n",
    "    est_flow_neigh = est_flow.view(bs * n, c)\n",
    "    est_flow_neigh = est_flow_neigh[NN_idx.view(bs * n, K)]\n",
    "\n",
    "    est_flow_neigh = est_flow_neigh[:, 1:K + 1, :]\n",
    "    flow_diff = est_flow.view(bs * n, c) - est_flow_neigh.permute(1, 0, 2)\n",
    "\n",
    "    flow_diff = (flow_diff).norm(p=loss_norm, dim=2)\n",
    "    smooth_flow_loss = flow_diff.mean()\n",
    "    smooth_flow_per_point = flow_diff.mean(dim=0).view(bs, n)\n",
    "\n",
    "    return smooth_flow_loss, smooth_flow_per_point\n",
    "\n",
    "# get normals\n",
    "normals1 = estimate_pointcloud_normals(pc1, neighborhood_size=K)\n",
    "\n",
    "pc_with_norms = torch.cat([pc1, normals1], dim=-1)\n",
    "_, nn_ind_norms, _ = knn_points(pc_with_norms, pc_with_norms, lengths1=None, lengths2=None, K=K, norm=1)\n",
    "\n",
    "smooth_loss, per_point = smoothness_loss(pc_with_norms, nn_ind_norms, loss_norm=1, mask=None)\n",
    "\n",
    "print(smooth_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:45.461283574Z",
     "start_time": "2023-06-12T13:58:45.433416417Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VIS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def mask_NN_by_dist(dist, nn_ind, max_radius):\n",
    "\n",
    "    tmp_idx = nn_ind[:, :, 0].unsqueeze(2).repeat(1, 1, K).to(nn_ind.device)\n",
    "    nn_ind[dist > max_radius] = tmp_idx[dist > max_radius]\n",
    "\n",
    "    return nn_ind\n",
    "\n",
    "vis_aware_nn_ind = strip_KNN_with_vis(pc1[0], nn_ind[0], VOF=VOF, HOF=HOF, margin=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:45.473178265Z",
     "start_time": "2023-06-12T13:58:45.459275410Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(0.1526),\n tensor([0.1264, 0.1170, 0.0881, 0.0648, 0.1744, 0.2216, 0.2953, 0.2737, 0.1324,\n         0.1848, 0.0982, 0.3610, 0.1031, 0.1185, 0.1607, 0.1485, 0.0874, 0.1753,\n         0.2533, 0.2075, 0.1591, 0.1518, 0.0965, 0.1356, 0.1071, 0.1191, 0.1765,\n         0.1496, 0.2589, 0.1643, 0.1347, 0.1229, 0.1099, 0.1183, 0.0884, 0.1432,\n         0.1535, 0.1019, 0.1640, 0.1553, 0.1960, 0.0668, 0.1665, 0.1698, 0.1143,\n         0.1272, 0.1737, 0.1524, 0.1758, 0.0824]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import FastGeodis\n",
    "\n",
    "class DT:\n",
    "    def __init__(self, pc1, pc2, grid_factor=10):\n",
    "        ''' works for batch size 1 only '''\n",
    "        self.grid_factor = grid_factor\n",
    "        pts = pc2[0]\n",
    "\n",
    "        pc1_min = torch.min(pc1.squeeze(0), 0)[0]\n",
    "        pc2_min = torch.min(pc2.squeeze(0), 0)[0]\n",
    "        pc1_max = torch.max(pc1.squeeze(0), 0)[0]\n",
    "        pc2_max = torch.max(pc2.squeeze(0), 0)[0]\n",
    "\n",
    "        xmin_int, ymin_int, zmin_int = torch.floor(torch.where(pc1_min < pc2_min, pc1_min, pc2_min) * 10 - 1) / 10\n",
    "        xmax_int, ymax_int, zmax_int = torch.ceil(torch.where(pc1_max > pc2_max, pc1_max, pc2_max) * 10 + 1) / 10\n",
    "\n",
    "        pmin = (xmin_int, ymin_int, zmin_int)\n",
    "        pmax = (xmax_int, ymax_int, zmax_int)\n",
    "\n",
    "        sample_x = ((pmax[0] - pmin[0]) * grid_factor).ceil().int() + 2\n",
    "        sample_y = ((pmax[1] - pmin[1]) * grid_factor).ceil().int() + 2\n",
    "        sample_z = ((pmax[2] - pmin[2]) * grid_factor).ceil().int() + 2\n",
    "\n",
    "        self.Vx = torch.linspace(0, sample_x, sample_x+1, device=pts.device)[:-1] / grid_factor + pmin[0]\n",
    "        self.Vy = torch.linspace(0, sample_y, sample_y+1, device=pts.device)[:-1] / grid_factor + pmin[1]\n",
    "        self.Vz = torch.linspace(0, sample_z, sample_z+1, device=pts.device)[:-1] / grid_factor + pmin[2]\n",
    "\n",
    "        # NOTE: build a binary image first, with 0-value occuppied points\n",
    "        grid_x, grid_y, grid_z = torch.meshgrid(self.Vx, self.Vy, self.Vz, indexing=\"ij\")\n",
    "        self.grid = torch.stack([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1), grid_z.unsqueeze(-1)], -1).float().squeeze()\n",
    "        H, W, D, _ = self.grid.size()\n",
    "        pts_mask = torch.ones(H, W, D, device=pts.device)\n",
    "        self.pts_sample_idx_x = ((pts[:,0:1] - self.Vx[0]) * self.grid_factor).round()\n",
    "        self.pts_sample_idx_y = ((pts[:,1:2] - self.Vy[0]) * self.grid_factor).round()\n",
    "        self.pts_sample_idx_z = ((pts[:,2:3] - self.Vz[0]) * self.grid_factor).round()\n",
    "        pts_mask[self.pts_sample_idx_x.long(), self.pts_sample_idx_y.long(), self.pts_sample_idx_z.long()] = 0.\n",
    "\n",
    "        iterations = 1\n",
    "        image_pts = torch.zeros(H, W, D, device=pts.device).unsqueeze(0).unsqueeze(0)\n",
    "        pts_mask = pts_mask.unsqueeze(0).unsqueeze(0)\n",
    "        self.D = FastGeodis.generalised_geodesic3d(\n",
    "            image_pts, pts_mask, [1./self.grid_factor, 1./self.grid_factor, 1./self.grid_factor], 1e10, 0.0, iterations\n",
    "        ).squeeze()\n",
    "\n",
    "    def torch_bilinear_distance(self, pc_deformed):\n",
    "\n",
    "        pc_deformed = pc_deformed.squeeze(0)\n",
    "\n",
    "        H, W, D = self.D.size()\n",
    "        target = self.D[None, None, ...]\n",
    "\n",
    "        sample_x = ((pc_deformed[:,0:1] - self.Vx[0]) * self.grid_factor).clip(0, H-1)\n",
    "        sample_y = ((pc_deformed[:,1:2] - self.Vy[0]) * self.grid_factor).clip(0, W-1)\n",
    "        sample_z = ((pc_deformed[:,2:3] - self.Vz[0]) * self.grid_factor).clip(0, D-1)\n",
    "\n",
    "        sample = torch.cat([sample_x, sample_y, sample_z], -1)\n",
    "\n",
    "        # NOTE: normalize samples to [-1, 1]\n",
    "        sample = 2 * sample\n",
    "        sample[...,0] = sample[...,0] / (H-1)\n",
    "        sample[...,1] = sample[...,1] / (W-1)\n",
    "        sample[...,2] = sample[...,2] / (D-1)\n",
    "        sample = sample -1\n",
    "\n",
    "        sample_ = torch.cat([sample[...,2:3], sample[...,1:2], sample[...,0:1]], -1)\n",
    "\n",
    "        # NOTE: reshape to match 5D volumetric input\n",
    "        dist = F.grid_sample(target, sample_.view(1,-1,1,1,3), mode=\"bilinear\", align_corners=True).view(-1)\n",
    "\n",
    "\n",
    "        return dist.mean(), dist\n",
    "\n",
    "\n",
    "# NOTE: build DT map\n",
    "dt = DT(pc1, pc2, grid_factor=10)\n",
    "dt.torch_bilinear_distance(pc1[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:45.565622798Z",
     "start_time": "2023-06-12T13:58:45.471909893Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Forward flow\n",
    "- forward flow with NN [x]\n",
    "- smoothness from NN(pc2,pc2) to NN(pc1+flow, pc2)\n",
    "- flow to same NN should be smooth or rigid kabsch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:45.566636592Z",
     "start_time": "2023-06-12T13:58:45.539143730Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1613/2860003869.py:21: UserWarning: scatter_reduce() is in beta and the API may change at any time. (Triggered internally at /local/temporary/eb-build/PyTorch/1.13.0/foss-2022a-CUDA-11.7.0/pytorch-v1.13.0/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1615.)\n",
      "  vec.scatter_reduce_(0, ind.repeat(1,3), a, reduce='mean', include_self=False)  # will do forward flow\n"
     ]
    }
   ],
   "source": [
    "def forward_flow_loss(pc1, pc2, est_flow):\n",
    "    ''' not yet for K > 1 or BS > 1\n",
    "    Smooth flow on same NN from pc2 '''\n",
    "    _, forward_nn, _ = knn_points(pc1 + est_flow, pc2, lengths1=None, lengths2=None, K=1, norm=1)\n",
    "\n",
    "    a = est_flow[0] # magnitude\n",
    "\n",
    "    ind = forward_nn[0] # more than one?\n",
    "\n",
    "    #if pc1 is bigger than pc2, then skip?\n",
    "    if pc1.shape[1] < pc2.shape[1]:\n",
    "        shape_diff = pc2.shape[1] - ind.shape[0] + 1 # one for dummy    # what if pc1 is bigger than pc2?\n",
    "        a = F.pad(a, (0,0,0, shape_diff), mode='constant', value=0)\n",
    "        a.retain_grad() # padding does not retain grad, need to do it manually. Check it\n",
    "\n",
    "        ind = F.pad(ind, (0,0,0, shape_diff), mode='constant', value=pc2.shape[1])  # pad with dummy not in orig\n",
    "\n",
    "    # storage of same points\n",
    "    vec = torch.zeros(ind.shape[0], 3)\n",
    "\n",
    "    vec.scatter_reduce_(0, ind.repeat(1,3), a, reduce='mean', include_self=False)  # will do forward flow\n",
    "\n",
    "    # loss\n",
    "    forward_loss = torch.nn.functional.mse_loss(vec[ind[:,0]], a, reduction='none').mean(dim=-1)\n",
    "\n",
    "    return forward_loss.mean(), forward_loss[:est_flow.shape[1]]\n",
    "\n",
    "f_loss, per_point_f_loss = forward_flow_loss(pc1, pc2, est_flow)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:45.567085991Z",
     "start_time": "2023-06-12T13:58:45.559519987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class UnsupervisedFlowLosses(torch.nn.Module):\n",
    "    # update samples\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        # store outputs of losses if better?\n",
    "        self.current_loss = 10000\n",
    "\n",
    "        # init losses\n",
    "        self.loss_functions = []\n",
    "\n",
    "        # [x] DT loss instead of chamfer\n",
    "        # [x] ForwardFlow\n",
    "        # todo push flow from freespace, mozna rikat visible-aware chamfer? []\n",
    "        # solver uprava\n",
    "        # kostka pro vyber NN / okoli\n",
    "\n",
    "\n",
    "    def update(self, pc1, pc2):\n",
    "\n",
    "        if args.loss_DT_weight > 0:\n",
    "            self.dt = DT(pc1, pc2, grid_factor=10)\n",
    "            # todo normals in DT?\n",
    "        if args.loss_smooth_normals:\n",
    "            normals1 = estimate_pointcloud_normals(pc1, neighborhood_size=args.loss_K_normals)\n",
    "            pc_with_norms = torch.cat([pc1, normals1], dim=-1)\n",
    "\n",
    "            # This is hence forward!\n",
    "            pc1 = pc_with_norms\n",
    "\n",
    "        if args.loss_smooth_weight > 0 and args.loss_vis_weight == 0:\n",
    "\n",
    "            _, self.nn_ind, _ = knn_points(pc1, pc1, lengths1=None, lengths2=None, K=args.loss_KNN_smooth, norm=1)\n",
    "\n",
    "        if args.loss_vis_weight > 0:\n",
    "            dist, nn_ind, _ = knn_points(pc1, pc1, lengths1=None, lengths2=None, K=args.loss_KNN_vis, norm=1)\n",
    "            mask_NN_by_dist(dist, nn_ind, args.loss_KNN_max_radius)\n",
    "            # BS = 1\n",
    "            self.nn_ind = strip_KNN_with_vis(pc1[0], nn_ind[0], VOF=VOF, HOF=HOF, margin=3).unsqueeze(0)\n",
    "\n",
    "\n",
    "    def forward(self, pc1, pc2, est_flow):\n",
    "\n",
    "        loss_dict = {}\n",
    "        loss = 0\n",
    "\n",
    "        if args.loss_smooth_weight > 0 or args.loss_vis_weight > 0:\n",
    "            smooth_loss, smooth_per_point = smoothness_loss(est_flow, self.nn_ind, loss_norm=1, mask=None)\n",
    "\n",
    "            loss_dict['smooth_loss'] = smooth_loss\n",
    "            loss_dict['smooth_per_point'] = smooth_per_point\n",
    "\n",
    "            loss += args.loss_smooth_weight * smooth_loss\n",
    "\n",
    "        if args.loss_chamfer_weight > 0 and args.loss_DT_weight == 0:\n",
    "            chamfer_loss, per_point_chamfer, x_to_y = chamfer_distance_loss(pc1 + est_flow, pc2, both_ways=args.loss_chamfer_both_ways, use_normals=args.loss_chamfer_use_normals, normals_K=args.loss_chamfer_normals_K, loss_norm=1)\n",
    "\n",
    "            loss_dict['chamfer_loss'] = chamfer_loss\n",
    "            loss_dict['chamfer_per_point'] = per_point_chamfer\n",
    "            loss_dict['chamfer_x_to_y'] = x_to_y\n",
    "\n",
    "            loss += args.loss_chamfer_weight * chamfer_loss\n",
    "\n",
    "        if args.loss_DT_weight > 0:\n",
    "            dt_loss, per_point_dt = self.dt.torch_bilinear_distance(pc1 + est_flow)\n",
    "\n",
    "            loss_dict['dt_loss'] = dt_loss\n",
    "            loss_dict['dt_per_point'] = per_point_dt\n",
    "\n",
    "            loss += args.loss_DT_weight * dt_loss\n",
    "\n",
    "        if args.loss_FF_weight > 0:\n",
    "            FF_loss, per_point_FF = forward_flow_loss(pc1, pc2, est_flow)\n",
    "\n",
    "            loss_dict['FF_loss'] = FF_loss\n",
    "            loss_dict['FF_per_point'] = per_point_FF\n",
    "\n",
    "            loss += args.loss_FF_weight * FF_loss\n",
    "\n",
    "        loss_dict['loss'] = loss\n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.loss_smooth_weight = 1.\n",
    "args.loss_KNN_smooth = 3\n",
    "args.loss_vis_weight = 0.\n",
    "args.loss_smooth_normals = 1\n",
    "args.loss_K_normals = 5\n",
    "args.loss_KNN_vis = 3\n",
    "args.loss_KNN_max_radius = 1.5\n",
    "\n",
    "args.loss_chamfer_weight = 1.\n",
    "args.loss_chamfer_both_ways = False\n",
    "args.loss_chamfer_use_normals = False\n",
    "args.loss_chamfer_normals_K = 5\n",
    "\n",
    "args.loss_DT_weight = 0.\n",
    "args.loss_DT_grid_factor = 10\n",
    "\n",
    "args.loss_FF_weight = 1.\n",
    "args.loss_FF_K = 1  # jine nema zatim\n",
    "\n",
    "Loss = UnsupervisedFlowLosses(args)\n",
    "Loss.update(pc1, pc2)\n",
    "loss_dict = Loss.forward(pc1, pc2, est_flow)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:45.616203814Z",
     "start_time": "2023-06-12T13:58:45.559718646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0352, grad_fn=<AddBackward0>)\n",
      "tensor(1.6313, grad_fn=<AddBackward0>)\n",
      "tensor(1.3237, grad_fn=<AddBackward0>)\n",
      "tensor(1.1034, grad_fn=<AddBackward0>)\n",
      "tensor(0.9652, grad_fn=<AddBackward0>)\n",
      "tensor(0.8808, grad_fn=<AddBackward0>)\n",
      "tensor(0.8059, grad_fn=<AddBackward0>)\n",
      "tensor(0.7510, grad_fn=<AddBackward0>)\n",
      "tensor(0.7133, grad_fn=<AddBackward0>)\n",
      "tensor(0.6693, grad_fn=<AddBackward0>)\n",
      "tensor(0.6134, grad_fn=<AddBackward0>)\n",
      "tensor(0.5664, grad_fn=<AddBackward0>)\n",
      "tensor(0.5197, grad_fn=<AddBackward0>)\n",
      "tensor(0.4953, grad_fn=<AddBackward0>)\n",
      "tensor(0.4830, grad_fn=<AddBackward0>)\n",
      "tensor(0.4887, grad_fn=<AddBackward0>)\n",
      "tensor(0.4726, grad_fn=<AddBackward0>)\n",
      "tensor(0.4544, grad_fn=<AddBackward0>)\n",
      "tensor(0.4376, grad_fn=<AddBackward0>)\n",
      "tensor(0.4334, grad_fn=<AddBackward0>)\n",
      "tensor(0.4235, grad_fn=<AddBackward0>)\n",
      "tensor(0.4091, grad_fn=<AddBackward0>)\n",
      "tensor(0.3985, grad_fn=<AddBackward0>)\n",
      "tensor(0.3833, grad_fn=<AddBackward0>)\n",
      "tensor(0.3668, grad_fn=<AddBackward0>)\n",
      "tensor(0.3603, grad_fn=<AddBackward0>)\n",
      "tensor(0.3590, grad_fn=<AddBackward0>)\n",
      "tensor(0.3577, grad_fn=<AddBackward0>)\n",
      "tensor(0.3442, grad_fn=<AddBackward0>)\n",
      "tensor(0.3254, grad_fn=<AddBackward0>)\n",
      "tensor(0.3093, grad_fn=<AddBackward0>)\n",
      "tensor(0.3185, grad_fn=<AddBackward0>)\n",
      "tensor(0.3230, grad_fn=<AddBackward0>)\n",
      "tensor(0.3174, grad_fn=<AddBackward0>)\n",
      "tensor(0.3097, grad_fn=<AddBackward0>)\n",
      "tensor(0.2998, grad_fn=<AddBackward0>)\n",
      "tensor(0.2963, grad_fn=<AddBackward0>)\n",
      "tensor(0.2915, grad_fn=<AddBackward0>)\n",
      "tensor(0.2826, grad_fn=<AddBackward0>)\n",
      "tensor(0.2841, grad_fn=<AddBackward0>)\n",
      "tensor(0.2804, grad_fn=<AddBackward0>)\n",
      "tensor(0.2729, grad_fn=<AddBackward0>)\n",
      "tensor(0.2730, grad_fn=<AddBackward0>)\n",
      "tensor(0.2694, grad_fn=<AddBackward0>)\n",
      "tensor(0.2633, grad_fn=<AddBackward0>)\n",
      "tensor(0.2596, grad_fn=<AddBackward0>)\n",
      "tensor(0.2633, grad_fn=<AddBackward0>)\n",
      "tensor(0.2575, grad_fn=<AddBackward0>)\n",
      "tensor(0.2621, grad_fn=<AddBackward0>)\n",
      "tensor(0.2617, grad_fn=<AddBackward0>)\n",
      "tensor(0.2588, grad_fn=<AddBackward0>)\n",
      "tensor(0.2546, grad_fn=<AddBackward0>)\n",
      "tensor(0.2520, grad_fn=<AddBackward0>)\n",
      "tensor(0.2519, grad_fn=<AddBackward0>)\n",
      "tensor(0.2550, grad_fn=<AddBackward0>)\n",
      "tensor(0.2486, grad_fn=<AddBackward0>)\n",
      "tensor(0.2538, grad_fn=<AddBackward0>)\n",
      "tensor(0.2528, grad_fn=<AddBackward0>)\n",
      "tensor(0.2509, grad_fn=<AddBackward0>)\n",
      "tensor(0.2516, grad_fn=<AddBackward0>)\n",
      "tensor(0.2489, grad_fn=<AddBackward0>)\n",
      "tensor(0.2548, grad_fn=<AddBackward0>)\n",
      "tensor(0.2512, grad_fn=<AddBackward0>)\n",
      "tensor(0.2487, grad_fn=<AddBackward0>)\n",
      "tensor(0.2526, grad_fn=<AddBackward0>)\n",
      "tensor(0.2499, grad_fn=<AddBackward0>)\n",
      "tensor(0.2444, grad_fn=<AddBackward0>)\n",
      "tensor(0.2491, grad_fn=<AddBackward0>)\n",
      "tensor(0.2456, grad_fn=<AddBackward0>)\n",
      "tensor(0.2455, grad_fn=<AddBackward0>)\n",
      "tensor(0.2443, grad_fn=<AddBackward0>)\n",
      "tensor(0.2438, grad_fn=<AddBackward0>)\n",
      "tensor(0.2471, grad_fn=<AddBackward0>)\n",
      "tensor(0.2409, grad_fn=<AddBackward0>)\n",
      "tensor(0.2399, grad_fn=<AddBackward0>)\n",
      "tensor(0.2431, grad_fn=<AddBackward0>)\n",
      "tensor(0.2461, grad_fn=<AddBackward0>)\n",
      "tensor(0.2451, grad_fn=<AddBackward0>)\n",
      "tensor(0.2434, grad_fn=<AddBackward0>)\n",
      "tensor(0.2449, grad_fn=<AddBackward0>)\n",
      "tensor(0.2446, grad_fn=<AddBackward0>)\n",
      "tensor(0.2425, grad_fn=<AddBackward0>)\n",
      "tensor(0.2432, grad_fn=<AddBackward0>)\n",
      "tensor(0.2458, grad_fn=<AddBackward0>)\n",
      "tensor(0.2479, grad_fn=<AddBackward0>)\n",
      "tensor(0.2445, grad_fn=<AddBackward0>)\n",
      "tensor(0.2386, grad_fn=<AddBackward0>)\n",
      "tensor(0.2492, grad_fn=<AddBackward0>)\n",
      "tensor(0.2473, grad_fn=<AddBackward0>)\n",
      "tensor(0.2466, grad_fn=<AddBackward0>)\n",
      "tensor(0.2460, grad_fn=<AddBackward0>)\n",
      "tensor(0.2474, grad_fn=<AddBackward0>)\n",
      "tensor(0.2495, grad_fn=<AddBackward0>)\n",
      "tensor(0.2446, grad_fn=<AddBackward0>)\n",
      "tensor(0.2426, grad_fn=<AddBackward0>)\n",
      "tensor(0.2459, grad_fn=<AddBackward0>)\n",
      "tensor(0.2487, grad_fn=<AddBackward0>)\n",
      "tensor(0.2457, grad_fn=<AddBackward0>)\n",
      "tensor(0.2449, grad_fn=<AddBackward0>)\n",
      "tensor(0.2423, grad_fn=<AddBackward0>)\n",
      "tensor(0.2429, grad_fn=<AddBackward0>)\n",
      "tensor(0.2464, grad_fn=<AddBackward0>)\n",
      "tensor(0.2459, grad_fn=<AddBackward0>)\n",
      "tensor(0.2463, grad_fn=<AddBackward0>)\n",
      "tensor(0.2462, grad_fn=<AddBackward0>)\n",
      "tensor(0.2447, grad_fn=<AddBackward0>)\n",
      "tensor(0.2437, grad_fn=<AddBackward0>)\n",
      "tensor(0.2452, grad_fn=<AddBackward0>)\n",
      "tensor(0.2439, grad_fn=<AddBackward0>)\n",
      "tensor(0.2466, grad_fn=<AddBackward0>)\n",
      "tensor(0.2454, grad_fn=<AddBackward0>)\n",
      "tensor(0.2469, grad_fn=<AddBackward0>)\n",
      "tensor(0.2467, grad_fn=<AddBackward0>)\n",
      "tensor(0.2413, grad_fn=<AddBackward0>)\n",
      "tensor(0.2406, grad_fn=<AddBackward0>)\n",
      "tensor(0.2422, grad_fn=<AddBackward0>)\n",
      "tensor(0.2413, grad_fn=<AddBackward0>)\n",
      "tensor(0.2435, grad_fn=<AddBackward0>)\n",
      "tensor(0.2441, grad_fn=<AddBackward0>)\n",
      "tensor(0.2445, grad_fn=<AddBackward0>)\n",
      "tensor(0.2422, grad_fn=<AddBackward0>)\n",
      "tensor(0.2398, grad_fn=<AddBackward0>)\n",
      "tensor(0.2368, grad_fn=<AddBackward0>)\n",
      "tensor(0.2404, grad_fn=<AddBackward0>)\n",
      "tensor(0.2426, grad_fn=<AddBackward0>)\n",
      "tensor(0.2412, grad_fn=<AddBackward0>)\n",
      "tensor(0.2403, grad_fn=<AddBackward0>)\n",
      "tensor(0.2388, grad_fn=<AddBackward0>)\n",
      "tensor(0.2373, grad_fn=<AddBackward0>)\n",
      "tensor(0.2383, grad_fn=<AddBackward0>)\n",
      "tensor(0.2373, grad_fn=<AddBackward0>)\n",
      "tensor(0.2429, grad_fn=<AddBackward0>)\n",
      "tensor(0.2408, grad_fn=<AddBackward0>)\n",
      "tensor(0.2406, grad_fn=<AddBackward0>)\n",
      "tensor(0.2365, grad_fn=<AddBackward0>)\n",
      "tensor(0.2405, grad_fn=<AddBackward0>)\n",
      "tensor(0.2431, grad_fn=<AddBackward0>)\n",
      "tensor(0.2396, grad_fn=<AddBackward0>)\n",
      "tensor(0.2389, grad_fn=<AddBackward0>)\n",
      "tensor(0.2394, grad_fn=<AddBackward0>)\n",
      "tensor(0.2371, grad_fn=<AddBackward0>)\n",
      "tensor(0.2427, grad_fn=<AddBackward0>)\n",
      "tensor(0.2394, grad_fn=<AddBackward0>)\n",
      "tensor(0.2434, grad_fn=<AddBackward0>)\n",
      "tensor(0.2421, grad_fn=<AddBackward0>)\n",
      "tensor(0.2380, grad_fn=<AddBackward0>)\n",
      "tensor(0.2392, grad_fn=<AddBackward0>)\n",
      "tensor(0.2437, grad_fn=<AddBackward0>)\n",
      "tensor(0.2450, grad_fn=<AddBackward0>)\n",
      "tensor(0.2426, grad_fn=<AddBackward0>)\n",
      "tensor(0.2439, grad_fn=<AddBackward0>)\n",
      "tensor(0.2427, grad_fn=<AddBackward0>)\n",
      "tensor(0.2436, grad_fn=<AddBackward0>)\n",
      "tensor(0.2399, grad_fn=<AddBackward0>)\n",
      "tensor(0.2385, grad_fn=<AddBackward0>)\n",
      "tensor(0.2376, grad_fn=<AddBackward0>)\n",
      "tensor(0.2431, grad_fn=<AddBackward0>)\n",
      "tensor(0.2384, grad_fn=<AddBackward0>)\n",
      "tensor(0.2378, grad_fn=<AddBackward0>)\n",
      "tensor(0.2411, grad_fn=<AddBackward0>)\n",
      "tensor(0.2407, grad_fn=<AddBackward0>)\n",
      "tensor(0.2411, grad_fn=<AddBackward0>)\n",
      "tensor(0.2386, grad_fn=<AddBackward0>)\n",
      "tensor(0.2437, grad_fn=<AddBackward0>)\n",
      "tensor(0.2392, grad_fn=<AddBackward0>)\n",
      "tensor(0.2407, grad_fn=<AddBackward0>)\n",
      "tensor(0.2478, grad_fn=<AddBackward0>)\n",
      "tensor(0.2434, grad_fn=<AddBackward0>)\n",
      "tensor(0.2407, grad_fn=<AddBackward0>)\n",
      "tensor(0.2466, grad_fn=<AddBackward0>)\n",
      "tensor(0.2421, grad_fn=<AddBackward0>)\n",
      "tensor(0.2386, grad_fn=<AddBackward0>)\n",
      "tensor(0.2434, grad_fn=<AddBackward0>)\n",
      "tensor(0.2447, grad_fn=<AddBackward0>)\n",
      "tensor(0.2464, grad_fn=<AddBackward0>)\n",
      "tensor(0.2436, grad_fn=<AddBackward0>)\n",
      "tensor(0.2446, grad_fn=<AddBackward0>)\n",
      "tensor(0.2456, grad_fn=<AddBackward0>)\n",
      "tensor(0.2422, grad_fn=<AddBackward0>)\n",
      "tensor(0.2378, grad_fn=<AddBackward0>)\n",
      "tensor(0.2484, grad_fn=<AddBackward0>)\n",
      "tensor(0.2464, grad_fn=<AddBackward0>)\n",
      "tensor(0.2401, grad_fn=<AddBackward0>)\n",
      "tensor(0.2430, grad_fn=<AddBackward0>)\n",
      "tensor(0.2437, grad_fn=<AddBackward0>)\n",
      "tensor(0.2384, grad_fn=<AddBackward0>)\n",
      "tensor(0.2460, grad_fn=<AddBackward0>)\n",
      "tensor(0.2457, grad_fn=<AddBackward0>)\n",
      "tensor(0.2388, grad_fn=<AddBackward0>)\n",
      "tensor(0.2465, grad_fn=<AddBackward0>)\n",
      "tensor(0.2513, grad_fn=<AddBackward0>)\n",
      "tensor(0.2465, grad_fn=<AddBackward0>)\n",
      "tensor(0.2434, grad_fn=<AddBackward0>)\n",
      "tensor(0.2532, grad_fn=<AddBackward0>)\n",
      "tensor(0.2498, grad_fn=<AddBackward0>)\n",
      "tensor(0.2441, grad_fn=<AddBackward0>)\n",
      "tensor(0.2438, grad_fn=<AddBackward0>)\n",
      "tensor(0.2509, grad_fn=<AddBackward0>)\n",
      "tensor(0.2483, grad_fn=<AddBackward0>)\n",
      "tensor(0.2429, grad_fn=<AddBackward0>)\n",
      "tensor(0.2453, grad_fn=<AddBackward0>)\n",
      "tensor(0.2483, grad_fn=<AddBackward0>)\n",
      "tensor(0.2500, grad_fn=<AddBackward0>)\n",
      "tensor(0.2464, grad_fn=<AddBackward0>)\n",
      "tensor(0.2472, grad_fn=<AddBackward0>)\n",
      "tensor(0.2449, grad_fn=<AddBackward0>)\n",
      "tensor(0.2502, grad_fn=<AddBackward0>)\n",
      "tensor(0.2494, grad_fn=<AddBackward0>)\n",
      "tensor(0.2459, grad_fn=<AddBackward0>)\n",
      "tensor(0.2453, grad_fn=<AddBackward0>)\n",
      "tensor(0.2484, grad_fn=<AddBackward0>)\n",
      "tensor(0.2484, grad_fn=<AddBackward0>)\n",
      "tensor(0.2502, grad_fn=<AddBackward0>)\n",
      "tensor(0.2459, grad_fn=<AddBackward0>)\n",
      "tensor(0.2430, grad_fn=<AddBackward0>)\n",
      "tensor(0.2490, grad_fn=<AddBackward0>)\n",
      "tensor(0.2490, grad_fn=<AddBackward0>)\n",
      "tensor(0.2440, grad_fn=<AddBackward0>)\n",
      "tensor(0.2462, grad_fn=<AddBackward0>)\n",
      "tensor(0.2469, grad_fn=<AddBackward0>)\n",
      "tensor(0.2444, grad_fn=<AddBackward0>)\n",
      "tensor(0.2436, grad_fn=<AddBackward0>)\n",
      "tensor(0.2411, grad_fn=<AddBackward0>)\n",
      "tensor(0.2412, grad_fn=<AddBackward0>)\n",
      "tensor(0.2446, grad_fn=<AddBackward0>)\n",
      "tensor(0.2406, grad_fn=<AddBackward0>)\n",
      "tensor(0.2386, grad_fn=<AddBackward0>)\n",
      "tensor(0.2394, grad_fn=<AddBackward0>)\n",
      "tensor(0.2402, grad_fn=<AddBackward0>)\n",
      "tensor(0.2441, grad_fn=<AddBackward0>)\n",
      "tensor(0.2411, grad_fn=<AddBackward0>)\n",
      "tensor(0.2431, grad_fn=<AddBackward0>)\n",
      "tensor(0.2432, grad_fn=<AddBackward0>)\n",
      "tensor(0.2415, grad_fn=<AddBackward0>)\n",
      "tensor(0.2408, grad_fn=<AddBackward0>)\n",
      "tensor(0.2408, grad_fn=<AddBackward0>)\n",
      "tensor(0.2447, grad_fn=<AddBackward0>)\n",
      "tensor(0.2425, grad_fn=<AddBackward0>)\n",
      "tensor(0.2472, grad_fn=<AddBackward0>)\n",
      "tensor(0.2478, grad_fn=<AddBackward0>)\n",
      "tensor(0.2427, grad_fn=<AddBackward0>)\n",
      "tensor(0.2444, grad_fn=<AddBackward0>)\n",
      "tensor(0.2462, grad_fn=<AddBackward0>)\n",
      "tensor(0.2383, grad_fn=<AddBackward0>)\n",
      "tensor(0.2459, grad_fn=<AddBackward0>)\n",
      "tensor(0.2442, grad_fn=<AddBackward0>)\n",
      "tensor(0.2497, grad_fn=<AddBackward0>)\n",
      "tensor(0.2433, grad_fn=<AddBackward0>)\n",
      "tensor(0.2432, grad_fn=<AddBackward0>)\n",
      "tensor(0.2441, grad_fn=<AddBackward0>)\n",
      "tensor(0.2472, grad_fn=<AddBackward0>)\n",
      "tensor(0.2443, grad_fn=<AddBackward0>)\n",
      "tensor(0.2449, grad_fn=<AddBackward0>)\n",
      "tensor(0.2420, grad_fn=<AddBackward0>)\n",
      "tensor(0.2397, grad_fn=<AddBackward0>)\n",
      "tensor(0.2458, grad_fn=<AddBackward0>)\n",
      "tensor(0.2472, grad_fn=<AddBackward0>)\n",
      "tensor(0.2420, grad_fn=<AddBackward0>)\n",
      "tensor(0.2452, grad_fn=<AddBackward0>)\n",
      "tensor(0.2436, grad_fn=<AddBackward0>)\n",
      "tensor(0.2425, grad_fn=<AddBackward0>)\n",
      "tensor(0.2481, grad_fn=<AddBackward0>)\n",
      "tensor(0.2486, grad_fn=<AddBackward0>)\n",
      "tensor(0.2436, grad_fn=<AddBackward0>)\n",
      "tensor(0.2457, grad_fn=<AddBackward0>)\n",
      "tensor(0.2468, grad_fn=<AddBackward0>)\n",
      "tensor(0.2455, grad_fn=<AddBackward0>)\n",
      "tensor(0.2440, grad_fn=<AddBackward0>)\n",
      "tensor(0.2485, grad_fn=<AddBackward0>)\n",
      "tensor(0.2478, grad_fn=<AddBackward0>)\n",
      "tensor(0.2476, grad_fn=<AddBackward0>)\n",
      "tensor(0.2465, grad_fn=<AddBackward0>)\n",
      "tensor(0.2416, grad_fn=<AddBackward0>)\n",
      "tensor(0.2425, grad_fn=<AddBackward0>)\n",
      "tensor(0.2449, grad_fn=<AddBackward0>)\n",
      "tensor(0.2420, grad_fn=<AddBackward0>)\n",
      "tensor(0.2438, grad_fn=<AddBackward0>)\n",
      "tensor(0.2490, grad_fn=<AddBackward0>)\n",
      "tensor(0.2450, grad_fn=<AddBackward0>)\n",
      "tensor(0.2453, grad_fn=<AddBackward0>)\n",
      "tensor(0.2503, grad_fn=<AddBackward0>)\n",
      "tensor(0.2496, grad_fn=<AddBackward0>)\n",
      "tensor(0.2468, grad_fn=<AddBackward0>)\n",
      "tensor(0.2460, grad_fn=<AddBackward0>)\n",
      "tensor(0.2458, grad_fn=<AddBackward0>)\n",
      "tensor(0.2496, grad_fn=<AddBackward0>)\n",
      "tensor(0.2470, grad_fn=<AddBackward0>)\n",
      "tensor(0.2412, grad_fn=<AddBackward0>)\n",
      "tensor(0.2456, grad_fn=<AddBackward0>)\n",
      "tensor(0.2508, grad_fn=<AddBackward0>)\n",
      "tensor(0.2484, grad_fn=<AddBackward0>)\n",
      "tensor(0.2419, grad_fn=<AddBackward0>)\n",
      "tensor(0.2465, grad_fn=<AddBackward0>)\n",
      "tensor(0.2484, grad_fn=<AddBackward0>)\n",
      "tensor(0.2494, grad_fn=<AddBackward0>)\n",
      "tensor(0.2483, grad_fn=<AddBackward0>)\n",
      "tensor(0.2466, grad_fn=<AddBackward0>)\n",
      "tensor(0.2484, grad_fn=<AddBackward0>)\n",
      "tensor(0.2482, grad_fn=<AddBackward0>)\n",
      "tensor(0.2456, grad_fn=<AddBackward0>)\n",
      "dict_keys(['smooth_loss', 'smooth_per_point', 'chamfer_loss', 'chamfer_per_point', 'chamfer_x_to_y', 'FF_loss', 'FF_per_point', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "# iterating over forward loss\n",
    "\n",
    "init_flow = est_flow.detach().clone()\n",
    "optimizer = torch.optim.Adam([est_flow], lr=0.1)\n",
    "\n",
    "Loss_Function = UnsupervisedFlowLosses(args)\n",
    "Loss_Function.update(pc1, pc2)\n",
    "\n",
    "for it in range(300):\n",
    "\n",
    "    loss_dict = Loss_Function(pc1, pc2, est_flow)\n",
    "\n",
    "    loss = loss_dict['loss']\n",
    "    loss.backward()\n",
    "\n",
    "    print(loss)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(loss_dict.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:46.319259533Z",
     "start_time": "2023-06-12T13:58:45.603095793Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from data.PATHS import TMP_VIS_PATH\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "color = cm.viridis(est_flow.norm(dim\n",
    "                                 =-1).detach().numpy()[0])\n",
    "\n",
    "plt.close()\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
    "axes[0].plot(init_flow[0,:,0].detach(), init_flow[0,:,1].detach(), 'bo')\n",
    "axes[0].plot(pc2[0,:,0].detach(), pc2[0,:,1].detach(), 'ro')\n",
    "\n",
    "s = axes[1].scatter(est_flow[0,:,0].detach(), est_flow[0,:,1].detach(), c=per_point_f_loss.detach(), cmap=cm.viridis)\n",
    "\n",
    "cb = plt.colorbar(mappable=s, ax=axes[1])\n",
    "cb.set_label('Cbar Label Here')\n",
    "\n",
    "# axes[1].grid(True, which='both', axis='x', linewidth=1)\n",
    "plt.savefig(TMP_VIS_PATH + 'scatter.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:46.723186202Z",
     "start_time": "2023-06-12T13:58:46.312055225Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "# transfer args to pandas, to csv and back\n",
    "\n",
    "import pandas as pd\n",
    "args_list = [args, args]\n",
    "values_list = []\n",
    "names = [n[0] for n in args_list[0]._get_kwargs()]\n",
    "\n",
    "for args in args_list:\n",
    "    values = [n[1] for n in args._get_kwargs()]\n",
    "    values_array = np.array((values))[None,:]\n",
    "    values_list.append(values_array)\n",
    "\n",
    "values_array = np.concatenate(values_list)\n",
    "\n",
    "df = pd.DataFrame(values_array, columns=names, index=None)\n",
    "df.to_csv('tst.csv')\n",
    "# rewrite manually?"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:58:46.923033511Z",
     "start_time": "2023-06-12T13:58:46.722484779Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Full loss example with animation\n",
    "- write example points for use case [x]\n",
    "- set up the losses for optimization of flow [x]\n",
    "- store iterations in video []\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# example points\n",
    "pc1 = torch.tensor(((1.3,1,0), (1.6,1,0), (2.4, 1, 0))).unsqueeze(0)\n",
    "pc2 = torch.tensor(((1.5,0.5,0), (1.8,0.6,0), (1.4, 0.2, 0), (1.6,0.2, 0), (1.5,0.2,0))).unsqueeze(0)\n",
    "est_flow = torch.rand(pc1.shape, requires_grad=True)\n",
    "est_flow.retain_grad()\n",
    "# est_flow.requires_grad_()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T15:29:14.120822064Z",
     "start_time": "2023-06-12T15:29:14.116958511Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(pc1[0,:,0], pc1[0,:,1], 'bo')\n",
    "plt.plot(pc2[0,:,0], pc2[0,:,1], 'ro')\n",
    "plt.quiver(pc1[0,:,0], pc1[0,:,1], est_flow[0,:,0].detach(), est_flow[0,:,1].detach(), color='g')\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.savefig(TMP_VIS_PATH + 'points.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T15:24:50.132113606Z",
     "start_time": "2023-06-12T15:24:49.924583622Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 1) FF loss\n",
    "_, forward_nn, _ = knn_points(pc1 + est_flow, pc2, lengths1=None, lengths2=None, K=1, norm=1)\n",
    "\n",
    "a = est_flow[0] # magnitude\n",
    "\n",
    "ind = forward_nn[0] # more than one?\n",
    "\n",
    "#if pc1 is bigger than pc2, then skip?\n",
    "if pc1.shape[1] < pc2.shape[1]:\n",
    "    shape_diff = pc2.shape[1] - ind.shape[0] + 1 # one for dummy    # what if pc1 is bigger than pc2?\n",
    "    a = F.pad(a, (0,0,0, shape_diff), mode='constant', value=0)\n",
    "    a.retain_grad() # padding does not retain grad, need to do it manually. Check it\n",
    "\n",
    "    ind = F.pad(ind, (0,0,0, shape_diff), mode='constant', value=pc2.shape[1])  # pad with dummy not in orig\n",
    "\n",
    "# storage of same points\n",
    "vec = torch.zeros(ind.shape[0], 3)\n",
    "\n",
    "vec = vec.scatter_reduce_(0, ind.repeat(1,3), a, reduce='mean', include_self=False)  # will do forward flow\n",
    "\n",
    "# print(ind)\n",
    "# print(vec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T15:29:16.389712441Z",
     "start_time": "2023-06-12T15:29:16.368770766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "# 2) smoothness propagation\n",
    "# K_SM = 3\n",
    "# _, NN_pc2, _ = knn_points(pc2, pc2, lengths1=None, lengths2=None, K=K_SM, norm=1)\n",
    "#\n",
    "# keep_ind = ind[ind[:,0] != pc2.shape[1] ,0]\n",
    "#\n",
    "# # print(\"\\n\\n Indices of pc2 for choosing only connections from NN(pc2) that has forward flow corespondences\\n\\n\", keep_ind)\n",
    "#\n",
    "#\n",
    "# # print(\"\\n\\n NN of pc2 that has correspondeces from flow\\n\\n\", NN_pc2[0, keep_ind, :])\n",
    "#\n",
    "# # znamena, ze est flow body maji tyhle indexy pro body v pc2 a ty indexy maji mit stejne flow.\n",
    "# n = NN_pc2[0, keep_ind, :]\n",
    "#\n",
    "# # beware of zeros\n",
    "# connected_flow = vec[n] # N x KSmooth x 3 (fx, fy, fz)\n",
    "# # print(connected_flow)\n",
    "#\n",
    "# # print(est_flow)\n",
    "# prep_flow = est_flow[0].unsqueeze(1).repeat_interleave(repeats=K_SM, dim=1)\n",
    "#\n",
    "# # smooth it, should be fine\n",
    "# flow_comparison = prep_flow - connected_flow\n",
    "#\n",
    "# # print(flow_comparison.shape)\n",
    "# # print(flow_comparison)\n",
    "# per_point_loss = flow_comparison.norm(dim=2).mean(dim=1)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T15:24:01.147764462Z",
     "start_time": "2023-06-12T15:24:01.131459675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8229, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [14], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m per_point_loss \u001B[38;5;241m=\u001B[39m flow_comparison\u001B[38;5;241m.\u001B[39mnorm(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m per_point_loss\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m---> 24\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     27\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m/mnt/appl/software/PyTorch/1.13.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mnt/appl/software/PyTorch/1.13.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# optimize\n",
    "K_SM = 3\n",
    "optimizer = torch.optim.Adam([est_flow], lr=0.1)\n",
    "_, NN_pc2, _ = knn_points(pc2, pc2, lengths1=None, lengths2=None, K=K_SM, norm=1)\n",
    "\n",
    "for e in range(10):\n",
    "\n",
    "    keep_ind = ind[ind[:,0] != pc2.shape[1] ,0]\n",
    "\n",
    "    # znamena, ze est flow body maji tyhle indexy pro body v pc2 a ty indexy maji mit stejne flow.\n",
    "    n = NN_pc2[0, keep_ind, :]\n",
    "\n",
    "    # beware of zeros\n",
    "    connected_flow = vec[n] # N x KSmooth x 3 (fx, fy, fz)\n",
    "\n",
    "    prep_flow = est_flow[0].unsqueeze(1).repeat_interleave(repeats=K_SM, dim=1)\n",
    "\n",
    "    # smooth it, should be fine\n",
    "    flow_comparison = prep_flow - connected_flow\n",
    "\n",
    "    per_point_loss = flow_comparison.norm(dim=2).mean(dim=1)\n",
    "    loss = per_point_loss.mean()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(loss)\n",
    "# todo solve this issue\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T15:29:19.042091207Z",
     "start_time": "2023-06-12T15:29:18.968695696Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
